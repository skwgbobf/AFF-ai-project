{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf9cd7f",
   "metadata": {},
   "source": [
    "# AFF Disease Prediction Using BEHRT for MLM\n",
    "\n",
    "This notebook demonstrates the training process for AFF disease prediction using BEHRT with Masked Language Modeling (MLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from common.common import create_folder\n",
    "from common.pytorch import load_model\n",
    "from dataLoader.MLM import MLMLoader\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from model.utils import age_vocab\n",
    "from model.MLM import BertForMaskedLM\n",
    "from model.optimiser import adam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffccaa1a",
   "metadata": {},
   "source": [
    "## Step 1: Define Model and Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define BERT Configuration Class\n",
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings=config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "\n",
    "# Training Configurations\n",
    "global_params = {\n",
    "    'max_seq_len': 64,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 3,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 256,\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cuda'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7081",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Data and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load preprocessed data\n",
    "with open('./T20_BFC_BEHRT_group_data_sickFinal_mlm_op1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "group_data_sickFinal_mlm = pd.DataFrame(data)\n",
    "\n",
    "# Load vocabulary\n",
    "with open('./vocab2_new.pkl', 'rb') as f:\n",
    "    vocab2 = pickle.load(f)\n",
    "\n",
    "# Create word dictionary\n",
    "word_dict = {'PAD': 0, 'CLS': 1, 'SEP': 2, 'MASK': 3, 'UNK': 4}\n",
    "for i, w in enumerate(vocab2):\n",
    "    word_dict[w] = i + 4\n",
    "BertVocab = word_dict\n",
    "\n",
    "# Create age vocabulary\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon=global_params['month'], symbol=global_params['age_symbol'])\n",
    "\n",
    "# Filter patients with sufficient visits\n",
    "data['length'] = data['d2'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "data = data[data['length'] >= global_params['min_visit']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cd105",
   "metadata": {},
   "source": [
    "## Step 3: Prepare DataLoader for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb5af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the DataLoader\n",
    "Dset = MLMLoader(data, BertVocab, ageVocab, max_len=train_params['max_len_seq'], code='d2', age='AGE2')\n",
    "trainload = torch.utils.data.DataLoader(dataset=Dset, batch_size=train_params['batch_size'], shuffle=True, num_workers=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41876b8d",
   "metadata": {},
   "source": [
    "## Step 4: Initialize BEHRT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define BEHRT Model Configuration\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab.keys()),\n",
    "    'hidden_size': 288,\n",
    "    'seg_vocab_size': 2,\n",
    "    'age_vocab_size': len(ageVocab.keys()),\n",
    "    'max_position_embedding': train_params['max_len_seq'],\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "    'num_hidden_layers': 6,\n",
    "    'num_attention_heads': 12,\n",
    "    'attention_probs_dropout_prob': 0.1,\n",
    "    'intermediate_size': 512,\n",
    "    'hidden_act': 'gelu',\n",
    "    'initializer_range': 0.02\n",
    "}\n",
    "\n",
    "conf = BertConfig(model_config)\n",
    "model = BertForMaskedLM(conf)\n",
    "model = model.to(train_params['device'])\n",
    "\n",
    "# Define Optimizer\n",
    "optim = adam(params=list(model.named_parameters()), config=optim_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b4c684",
   "metadata": {},
   "source": [
    "## Step 5: Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cal_acc(label, pred):\n",
    "    logs = nn.LogSoftmax(dim=-1)\n",
    "    label = label.cpu().numpy()\n",
    "    ind = np.where(label != -1)[0]\n",
    "    truepred = pred.detach().cpu().numpy()\n",
    "    truepred = truepred[ind]\n",
    "    truelabel = label[ind]\n",
    "    truepred = logs(torch.tensor(truepred))\n",
    "    outs = [np.argmax(pred_x) for pred_x in truepred.numpy()]\n",
    "    precision = skm.precision_score(truelabel, outs, average='micro')\n",
    "    recall = skm.recall_score(truelabel, outs, average='micro')\n",
    "    f1 = skm.f1_score(truelabel, outs, average='micro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "def train(e, loader):\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    start = time.time()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = tuple(t.to(train_params['device']) for t in batch)\n",
    "        age_ids, input_ids, posi_ids, segment_ids, attMask, masked_label = batch\n",
    "        loss, pred, label = model(input_ids, age_ids, segment_ids, posi_ids, attention_mask=attMask, masked_lm_labels=masked_label)\n",
    "\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss / global_params['gradient_accumulation_steps']\n",
    "        loss.backward()\n",
    "\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            precision, recall, f1 = cal_acc(label, pred)\n",
    "            print(f\"Epoch: {e}, Step: {step}, Loss: {temp_loss/200:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            temp_loss = 0\n",
    "\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    return tr_loss / len(loader), time.time() - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8206e94",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "file_config = {\n",
    "    'model_path': './',\n",
    "    'model_name': 'T20_BFC_MLM_3_op1_v1',\n",
    "    'file_name': 'MLM_log_3_op1'\n",
    "}\n",
    "\n",
    "create_folder(file_config['model_path'])\n",
    "\n",
    "f = open(os.path.join(file_config['model_path'], file_config['file_name']), \"w\")\n",
    "f.write('{}\\t{}\\t{}\\n'.format('epoch', 'loss', 'time'))\n",
    "\n",
    "for e in range(30):\n",
    "    loss, time_cost = train(e, trainload)\n",
    "    f.write('{}\\t{:.4f}\\t{:.2f}\\n'.format(e, loss, time_cost))\n",
    "\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
